**Value Iteration** does not fully evaluate the policy at each step; instead, it represents the other extreme by performing only one evaluation update per iteration. Intuitively, this makes sense and should be faster: it uses the Bellman optimality equation as the iteration formula, rather than the Bellman expectation equation, directly targeting our goal of finding the optimal policy. This approach saves time by avoiding the extensive evaluation of intermediate, non-optimal policies.