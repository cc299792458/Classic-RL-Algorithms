**Gradient Monte Carlo** is a reinforcement learning method that directly updates the policy parameters using gradient ascent based on the return from Monte Carlo rollouts. It relies on full episodes to compute the expected return and uses the policy gradient theorem to adjust the policy in the direction that maximizes the expected reward. Unlike Temporal-Difference methods, Gradient Monte Carlo updates are made only after an entire episode is completed, resulting in unbiased but high-variance estimates.