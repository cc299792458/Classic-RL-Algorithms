**Dyna Q** is a reinforcement learning algorithm that combines real-world experience with simulated planning using a model. It updates the policy from actual interactions and additional simulated steps, improving learning efficiency by reusing data. More planning steps increase efficiency but also computational cost, requiring a balance.

In the book, an interesting example is mentioned: during the first episode, Dyna Q only updates the last state. However, by the second episode, it quickly learns the actions for tons of states (even if they are not yet optimal). The first phenomenon occurs because, for one-step updates, meaningful updates require at least one state to have received a reward, which only happens at the final step. The second phenomenon happens because, although the last step in the first episode was the only one that updated the Q-function, the model itself was updated throughout the episode. Thus, in the second episode, many state values were updated thanks to the model. This shows that with a model, more information can be extracted and utilized from interactions with the environment.